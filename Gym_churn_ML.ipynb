{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef5b6631-bc4b-4429-9f15-3d68e8affc60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load data with selected columns\n",
    "df = pd.read_csv('gym_churn_us.csv', usecols=[\n",
    "    'gender', 'Near_Location', 'Partner', 'Promo_friends', 'Phone',\n",
    "    'Contract_period', 'Group_visits', 'Age', 'Avg_additional_charges_total',\n",
    "    'Month_to_end_contract', 'Lifetime', 'Avg_class_frequency_total',\n",
    "    'Avg_class_frequency_current_month', 'Churn'\n",
    "])\n",
    "\n",
    "# Handle missing values if necessary (here filling with mean for numerical columns)\n",
    "df.fillna(df.mean(), inplace=True)\n",
    "\n",
    "# Convert categorical columns to dummy variables\n",
    "df = pd.get_dummies(df, drop_first=True)\n",
    "\n",
    "# Split data into features (X) and target (y)\n",
    "X = df.drop('Churn', axis=1)\n",
    "y = df['Churn']\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "93e8114d-6afd-4dcf-8f69-419fefb4c2af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9141666666666667\n",
      "ROC AUC: 0.871557557093502\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.96      0.94       897\n",
      "           1       0.86      0.79      0.82       303\n",
      "\n",
      "    accuracy                           0.91      1200\n",
      "   macro avg       0.90      0.87      0.88      1200\n",
      "weighted avg       0.91      0.91      0.91      1200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score, roc_auc_score\n",
    "\n",
    "# Train a random forest model\n",
    "model = RandomForestClassifier(random_state=42)\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "roc_auc = roc_auc_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"ROC AUC: {roc_auc}\")\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "24f6528c-4365-4e41-ba96-ffaa2c08701e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n",
      "[CV 1/3] END max_depth=None, min_samples_split=2, n_estimators=100;, score=0.910 total time=   2.0s\n",
      "[CV 2/3] END max_depth=None, min_samples_split=2, n_estimators=100;, score=0.925 total time=   1.3s\n",
      "[CV 3/3] END max_depth=None, min_samples_split=2, n_estimators=100;, score=0.909 total time=   1.3s\n",
      "[CV 1/3] END max_depth=None, min_samples_split=2, n_estimators=200;, score=0.914 total time=   2.8s\n",
      "[CV 2/3] END max_depth=None, min_samples_split=2, n_estimators=200;, score=0.930 total time=   2.8s\n",
      "[CV 3/3] END max_depth=None, min_samples_split=2, n_estimators=200;, score=0.908 total time=   2.7s\n",
      "[CV 1/3] END max_depth=None, min_samples_split=5, n_estimators=100;, score=0.911 total time=   1.3s\n",
      "[CV 2/3] END max_depth=None, min_samples_split=5, n_estimators=100;, score=0.925 total time=   1.3s\n",
      "[CV 3/3] END max_depth=None, min_samples_split=5, n_estimators=100;, score=0.910 total time=   1.8s\n",
      "[CV 1/3] END max_depth=None, min_samples_split=5, n_estimators=200;, score=0.910 total time=   4.1s\n",
      "[CV 2/3] END max_depth=None, min_samples_split=5, n_estimators=200;, score=0.920 total time=   3.4s\n",
      "[CV 3/3] END max_depth=None, min_samples_split=5, n_estimators=200;, score=0.913 total time=   3.0s\n",
      "[CV 1/3] END max_depth=10, min_samples_split=2, n_estimators=100;, score=0.910 total time=   2.6s\n",
      "[CV 2/3] END max_depth=10, min_samples_split=2, n_estimators=100;, score=0.928 total time=   1.4s\n",
      "[CV 3/3] END max_depth=10, min_samples_split=2, n_estimators=100;, score=0.912 total time=   1.3s\n",
      "[CV 1/3] END max_depth=10, min_samples_split=2, n_estimators=200;, score=0.910 total time=   2.7s\n",
      "[CV 2/3] END max_depth=10, min_samples_split=2, n_estimators=200;, score=0.924 total time=   2.9s\n",
      "[CV 3/3] END max_depth=10, min_samples_split=2, n_estimators=200;, score=0.911 total time=   3.2s\n",
      "[CV 1/3] END max_depth=10, min_samples_split=5, n_estimators=100;, score=0.909 total time=   1.7s\n",
      "[CV 2/3] END max_depth=10, min_samples_split=5, n_estimators=100;, score=0.920 total time=   1.8s\n",
      "[CV 3/3] END max_depth=10, min_samples_split=5, n_estimators=100;, score=0.911 total time=   1.4s\n",
      "[CV 1/3] END max_depth=10, min_samples_split=5, n_estimators=200;, score=0.907 total time=   2.9s\n",
      "[CV 2/3] END max_depth=10, min_samples_split=5, n_estimators=200;, score=0.921 total time=   2.9s\n",
      "[CV 3/3] END max_depth=10, min_samples_split=5, n_estimators=200;, score=0.909 total time=   2.9s\n",
      "Best parameters: {'max_depth': None, 'min_samples_split': 2, 'n_estimators': 200}\n",
      "Best score: 0.9175011265112273\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define a grid of hyperparameters\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'max_depth': [None, 10],\n",
    "    'min_samples_split': [2, 5]\n",
    "}\n",
    "\n",
    "# Set up grid search\n",
    "grid_search = GridSearchCV(RandomForestClassifier(random_state=42), param_grid, cv=3, scoring='accuracy', verbose=3)\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "print(f\"Best score: {grid_search.best_score_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "accac7b7-5658-439e-8105-59da4acb714d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gender: 0.011145441891131631\n",
      "Near_Location: 0.010750289176833572\n",
      "Partner: 0.012039458047032204\n",
      "Promo_friends: 0.011125006841781377\n",
      "Phone: 0.006728057257887894\n",
      "Contract_period: 0.06653511937954595\n",
      "Group_visits: 0.015828532982270906\n",
      "Age: 0.13518884872414122\n",
      "Avg_additional_charges_total: 0.08254894733618195\n",
      "Month_to_end_contract: 0.08080831168655343\n",
      "Lifetime: 0.2766897073752608\n",
      "Avg_class_frequency_total: 0.12398645170522567\n",
      "Avg_class_frequency_current_month: 0.16662582759615344\n"
     ]
    }
   ],
   "source": [
    "# Feature importance in Random Forest\n",
    "feature_importance = model.feature_importances_\n",
    "features = X.columns\n",
    "\n",
    "# Print the importance of each feature\n",
    "for feature, importance in zip(features, feature_importance):\n",
    "    print(f\"{feature}: {importance}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "48bf0890-e7e2-48cf-a547-be5f0abd2016",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['gym_churn_model.pkl']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "# Save the model to a file\n",
    "joblib.dump(model, 'gym_churn_model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "395d1452-c7e3-452e-b09e-4db3460707e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming you have these variables from the previous analysis\n",
    "# - X_test: the test dataset features\n",
    "# - y_test: the actual labels (ground truth)\n",
    "# - y_pred: the predicted values\n",
    "# - feature_importance: the feature importance values\n",
    "# - features: the feature names\n",
    "\n",
    "# Convert predictions to DataFrame\n",
    "df_predictions = pd.DataFrame(X_test, columns=features)\n",
    "df_predictions['Actual_Churn'] = y_test\n",
    "df_predictions['Predicted_Churn'] = y_pred\n",
    "\n",
    "# If you want to include feature importance (optional)\n",
    "df_feature_importance = pd.DataFrame({\n",
    "    'Feature': features,\n",
    "    'Importance': feature_importance\n",
    "}).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Now you can combine all relevant information into a final DataFrame\n",
    "final_df = df_predictions.copy()\n",
    "\n",
    "# Save the final DataFrame to a CSV for Tableau\n",
    "final_df.to_csv('final_gym_churn_analysis.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d2867d-fc68-46d4-b0a0-cbd2e60a90f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
